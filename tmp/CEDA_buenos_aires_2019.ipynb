{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice for the estimation of Q and R in data assimilation\n",
    "\n",
    "This Jupyter notebook is part of the intensive course on data assimilation and filtering techniques that held in Buenos Aires in October 2019. It uses the **pykalman** and **CEDA** Python libraries respectively available at https://github.com/pykalman and https://github.com/ptandeo/CEDA. CEDA stands for \"Covariance Estimation in Data Assimilation\". Do not hesitate to contact me (pierre.tandeo@imt-atlantique.fr) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM STATEMENT\n",
    "\n",
    "Data assimilation are numerical methods used in geophysics to mix the information of observations (noted as $y$) and a dynamical model (noted as $\\mathcal{M}$). The goal is to estimate the true/hidden state of the system (noted as $x$) at every time step $k$. Usually, data assimilation problems are stated as nonlinear state-space models:\n",
    "\n",
    "\\begin{align*}\n",
    "x(k) & = \\mathcal{M}\\left(x(k-1)\\right) + \\eta(k) \\\\\n",
    "y(k) & = \\mathcal{H}\\left(x(k)\\right) + \\epsilon(k)\n",
    "\\end{align*}\n",
    "\n",
    "with $\\eta$ and $\\epsilon$ some independent white Gaussian noises respectively representing the model and the observation errors. These errors are supposed to be unbiased with covariances noted $Q$ and $R$. Here, we propose to use the Expectation-Maximization (EM) algorithm to estimate $Q$ and $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The batch EM algorithm for state-space models\n",
    "\n",
    "EM algorithm is a common algorithm in the statistical society. The goal of EM is to maximize a likelihood function. In our case (state-space models), the likelihood function to maximize is \n",
    "\\begin{align*}\n",
    "%& p\\left(y(1:K), x(0:K)|Q, R \\right) = & \\nonumber \\\\ \n",
    "& \\mathcal{L}(Q,R) = p\\left(x(0)\\right) \\prod_{k=1}^K p\\left(x(k)|x(k-1),Q\\right) \\prod_{k=1}^K p\\left(y(k)|x(k),R\\right) & \n",
    "\\end{align*}\n",
    "where the first term corresponds to the initial condition, the second term to the dynamic equation, the third term to the observation equation. Note that $K$ is the total number of observations.\n",
    "\n",
    "The $\\mathcal{L}(Q,R)$ likelihood is said to be \"total\" because we consider the all state-space model at all time steps. In practice, we can not maximize directly this likelihood w.r.t. $Q$ and $R$ because the sequence $x(0),\\dots,x(K)$ is unknown and depends itself on $Q$ and $R$. We thus need an iterative procedure to first estimate the sequence $x(0),\\dots,x(K)$ and then to update $Q$ and $R$. This is the purpose of the EM algorithm.\n",
    "\n",
    "The 2 main steps in EM are (1) the Expectation and (2) the Maximization. EM algorithm is iterative and we note $j$ the current iteration. Starting from an initial condition $Q^{(0)}$ and $R^{(0)}$, EM repeats these 2 steps until converge:\n",
    "\n",
    "1) E-step: compute the \"expectation\" of the likelihood function conditionally on the previous estimates $Q^{(j-1)}$ and $R^{(j-1)}$. Mathematically, it is written as $E\\left[\\mathcal{L}(Q,R)|y(0),\\dots,y(K),Q^{(j-1)},R^{(j-1)}\\right]$. This expectation is also conditionally on the whole sequence of observations $y(0),\\dots,y(K)$. We thus need to use the Kalman smoother to approximate as best this expectation.\n",
    "\n",
    "2) M-step: \"maximize\" $E\\left[\\mathcal{L}(Q,R)|y(0),\\dots,y(K),Q^{(j-1)},R^{(j-1)}\\right]$ w.r.t. $Q$ and $R$. This maximization can be done using optimization techniques (e.g., gradient descent) or using an analytic formula. The resulting covariance estimates are $Q^{(j)}$ and $R^{(j)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm using the linear and Gaussian state-space model\n",
    "\n",
    "Here, we consider the case where $\\mathcal{M}$ and $\\mathcal{H}$ are linear operators (i.e. $M$ and $H$ matrices). The EM for the corresponding linear and Gaussian state-space model is known for a long time and given in Shumway and Stoffer (1982), see https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1467-9892.1982.tb00349.x. This algorithm is coded in the **pykalman** Python library. Here, we aim to apply the EM algorithm on a simple problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question)** Generate data from the following linear and Gaussian state-space model:\n",
    "\\begin{align}\n",
    "  x(k) & = 0.95 x(k-1) + \\eta(k)\\\\\n",
    "  y(k) & = x(k) + \\epsilon(k)\n",
    "\\end{align}\n",
    "with $\\eta(k) \\sim \\mathcal{N}\\left(0,Q^{true}=1\\right)$ and $\\epsilon(k) \\sim \\mathcal{N}\\left(0,R^{true}=1\\right)$. Use the function *random.normal* to generate the Gaussian noises. Then, plot the $x$ and $y$ time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 9)\n",
    "numpy.random.seed(5)\n",
    "\n",
    "# generate true state and noisy observations\n",
    "Q_true=eye(1)\n",
    "R_true=eye(1)\n",
    "K=100\n",
    "x_true=zeros(K)\n",
    "y=zeros(K)\n",
    "for k in range(K):\n",
    "    x_true[k]= ### TO DO ###\n",
    "    y[k]= ### TO DO ###\n",
    "\n",
    "# plot results\n",
    "figure()\n",
    "line1,=plot(range(K),x_true,'r',linewidth=2)\n",
    "line2,=plot(range(K),y,'.k')\n",
    "legend([line1, line2], ['True state $x$', 'Noisy observations $y$'], prop={'size': 20})\n",
    "ylim(min(y)-1,max(y)+1)\n",
    "title('Simulated data from a linear Gaussian state-space model', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the Kalman smoother using the true parameters: $M=0.95$, $H=1$, $Q=Q^{true}$ and $R=R^{true}$. We plot the corresponding results. The estimated state using Kalman is close to the truth and the $95\\%$ confidence interval seems realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for Kalman filter/smoother and EM algorithm (linear case)\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# apply Kalman smoother with good covariances\n",
    "KF_true_Q_R=KalmanFilter(transition_matrices = [[0.95]], observation_matrices = [[1]],\n",
    "                         initial_state_mean = [[0]], initial_state_covariance = [[1]],\n",
    "                         transition_covariance = Q_true, observation_covariance = R_true)\n",
    "xs_true_Q_R, Ps_true_Q_R=KF_true_Q_R.smooth(y)\n",
    "\n",
    "# plot results\n",
    "figure()\n",
    "line1,=plot(range(K),x_true,'r',linewidth=2)\n",
    "line2,=plot(range(K),y,'.k')\n",
    "line3,=plot(range(K),xs_true_Q_R,'k',linewidth=2)\n",
    "fill_between(range(K), squeeze(xs_true_Q_R) - 1.96 * sqrt(squeeze(Ps_true_Q_R)),\n",
    "             squeeze(xs_true_Q_R) + 1.96 * sqrt(squeeze(Ps_true_Q_R)), color='k', alpha=.2)\n",
    "legend([line1, line2, line3], ['True state $x$', 'Noisy observations $y$', 'Estimated state'], prop={'size': 20})\n",
    "ylim(min(y)-1,max(y)+1)\n",
    "title('Results of the Kalman smoother', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question)** Now, apply the Kalman smoother changing the values of $Q$ and $R$. Firstly, you can fix $R=R^{true}=1$ and vary $Q$. Secondly, you can fix $Q=Q^{true}=1$ and vary $R$. Thirdly, you can change both $Q$ and $R$. What are your feelings about the results, both in term of estimated mean ($x^s$) and variance ($P^s$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above clearly show that the quality of the estimation highly depends on $Q$ and $R$. But in practice, we don't know the optimal values. A solution is to use the batch EM algorithm to estimate automatically these quantities. This is implemented in the *em* function of the **pykalman** library. First, we initialize the state-space model with arbitrary $Q^{(0)}$ and $R^{(0)}$ values. Then, we run the EM algorithm with a given number of iterations. Finally, we print the estimated values of $Q$ and $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the state-space model\n",
    "KF_estimate_Q_R=KalmanFilter(transition_matrices = [[0.95]], observation_matrices = [[1]],\\\n",
    "                             initial_state_mean = [[0]], initial_state_covariance = [[1]],\\\n",
    "                             transition_covariance = 0.5*Q_true,\\\n",
    "                             observation_covariance = 5*R_true)\n",
    "\n",
    "# apply EM algorithm (for only Q and R)\n",
    "KF_estimate_Q_R.em(y, n_iter=100, em_vars=['transition_covariance','observation_covariance'])\n",
    "\n",
    "# plot Q and R estimates\n",
    "print('Q estimates: ', KF_estimate_Q_R.transition_covariance)\n",
    "print('R estimates: ', KF_estimate_Q_R.observation_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question)** Are the estimation of $Q$ and $R$ correct? If yes, great! If no, can you explain why? Can you apply the EM algorithm with another EM configuration to improve the estimation results? See https://pykalman.github.io/#optimizing-parameters for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the estimation of $Q$ and $R$ can be improved using the batch EM algorithm. The first option is to estimate the initial condition of the state, i.e. $x(0)$ and $P(0)$, within the EM. In that case, we avoid the well-known \"spin-up effect\". The second option is to increase the size of the observation sequence. In that case, the likelihood function $\\mathcal{L}(Q,R)$ is taking into account more observations and the $Q$ and $R$ estimates are more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm using the nonlinear and Gaussian state-space model\n",
    "\n",
    "Above, we were dealing with a simple case: 1-dimensional problem and linear dynamic model. Below, we will work with the 3-dimensional and nonlinear dynamic model, namely the Lorenz-63 system. In that case, we can not use anymore the linear Kalman filter/smoother. Instead, we use the ensemble Kalman filter/smoother (noted EnKF and EnKS). These data assimilation algorithms use $Ne$ ensembles to empirically estimate the mean and covariance of the state. The smoothing members for a given time step $k$ are denoted by $x^s_1(k), \\dots, x^s_{Ne}(k)$. \n",
    "\n",
    "\n",
    "In the nonlinear and Gaussian case, EM algorithm is slightly different but quite intuitive (see https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3048). Indeed, the $Q^{(j)}$ and $R^{(j)}$ estimates in the \"maximization\" step are respectively given by:\n",
    "\n",
    "\\begin{align*}\n",
    "Q^{(j)} & = \\frac{1}{K Ne} \\sum_{k=1}^K \\sum_{i=1}^{Ne} \\left[ x^s_i(k) - \\mathcal{M}\\left( x^s_i(k-1) \\right) \\right] \\left[ x^s_i(k) - \\mathcal{M}\\left( x^s_i(k-1) \\right) \\right]^\\top\\\\\n",
    "R^{(j)} & = \\frac{1}{K Ne} \\sum_{k=1}^K \\sum_{i=1}^{Ne} \\left[ y(k) - \\mathcal{H}\\left( x^s_i(k) \\right) \\right] \\left[ y(k) - \\mathcal{H}\\left( x^s_i(k) \\right) \\right]^\\top\n",
    "\\end{align*}\n",
    "\n",
    "with $x^s_i(k)$, for all members $i$ and time steps $k$, the results of EnKS with the previous estimates $Q^{(j-1)}$ and $R^{(j-1)}$. EM algorithm using EnKS is coded in the **CEDA** Python library. Here, we aim to apply this EM algorithm on the Lorenz-63 system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question)** Generate data from the following nonlinear and Gaussian state-space model:\n",
    "\\begin{align*}\n",
    "x(k) & = \\mathcal{M}\\left(x(k-1)\\right) + \\eta(k) \\\\\n",
    "y(k) & = x(k) + \\epsilon(k)\n",
    "\\end{align*}\n",
    "with $\\mathcal{M}$ the Lorenz equations, $\\eta(k) \\sim \\mathcal{N}\\left(0,Q^{true}=0.05 I\\right)$ and $\\epsilon(k) \\sim \\mathcal{N}\\left(0,R^{true}=2I\\right)$. Here, we use $d_t=0.01$ as the integration step. We use the functions *gen_truth* and *gen_obs* to generate $x$ and $y$. Then, we plot the $x$ and $y$ time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CEDA library\n",
    "from numpy.random import RandomState\n",
    "from models.L63 import l63_predict\n",
    "from algos.EM_EnKS import EnKS, EM_EnKS\n",
    "from algos.utils import gen_truth, gen_obs, climat_background\n",
    "\n",
    "# generate true state \n",
    "dt = .01 # integration time\n",
    "sigma = 10;rho = 28;beta = 8./3 # physical parameters of Lorenz-63\n",
    "m = lambda x: l63_predict(x, dt, sigma, rho, beta) # dynamic operator\n",
    "h = lambda x: x # observation operator (nonlinear version)\n",
    "H = eye(3) # observation operator (linear version)\n",
    "K = 1000\n",
    "Q_true = 0.05*eye(3)\n",
    "x_true = gen_truth(m, r_[6.39435776, 9.23172442, 19.15323224], K, Q_true, RandomState(5))\n",
    "\n",
    "# generate noisy observations\n",
    "dt_obs = 5 # 1 observation every dt_obs time steps\n",
    "R_true = 2*eye(3)\n",
    "y = gen_obs(h, x_true, R_true, dt_obs, RandomState(5))\n",
    "\n",
    "# plot results\n",
    "figure()\n",
    "line1,=plot(range(K),x_true[0,1:],'r',linewidth=2)\n",
    "line2,=plot(range(K),y[0,:],'.k')\n",
    "legend([line1, line2], ['True state $x$', 'Noisy observations $y$'], prop={'size': 20})\n",
    "title('Simulated data from the Lorenz-63 model (only the first component)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the EnKS using the true parameters: $Q=Q^{true}$ and $R=R^{true}$. We plot the corresponding results. The estimated state using Kalman is close to the truth and the $95\\%$ confidence interval seems realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply EnKS with good covariances\n",
    "params = { 'state_size'                  : 3,\n",
    "           'nb_particles'                : 100,\n",
    "           'temporal_window_size'        : K,\n",
    "           'observation_matrix'          : H,\n",
    "           'observation_noise_covariance': R_true,\n",
    "           'observations'                : y,\n",
    "           'true_state'                  : x_true,\n",
    "           'observation_size'            : 3,\n",
    "           'background_state'            : r_[6.39435776, 9.23172442, 19.15323224],\n",
    "           'background_covariance'       : eye(3),\n",
    "           'model_noise_covariance'      : Q_true,\n",
    "           'inflation_factor'            : 1,\n",
    "           'model_dynamics'              : m}\n",
    "EnKS_true_Q_R=EnKS(params, RandomState(5))\n",
    "ens_true_Q_R=EnKS_true_Q_R['smoothed_ensemble'] # smoother ensembles\n",
    "xs_true_Q_R=mean(EnKS_true_Q_R['smoothed_ensemble'], 1)\n",
    "Ps_true_Q_R=diag(cov(ens_true_Q_R[0,:,:], rowvar=False))\n",
    "\n",
    "# plot results\n",
    "figure()\n",
    "line1,=plot(range(K),x_true[0,1:],'r',linewidth=2)\n",
    "line2,=plot(range(K),y[0,:],'.k')\n",
    "line3,=plot(range(K),xs_true_Q_R[0,1:],'k',linewidth=2)\n",
    "fill_between(range(K), squeeze(xs_true_Q_R[0,1:]) - 1.96 * sqrt(squeeze(Ps_true_Q_R[1:])),\n",
    "             squeeze(xs_true_Q_R[0,1:]) + 1.96 * sqrt(squeeze(Ps_true_Q_R[1:])), color='k', alpha=.2)\n",
    "legend([line1, line2, line3], ['True state $x$', 'Noisy observations $y$', 'Estimated state'], prop={'size': 20})\n",
    "title('Results of the EnKS (only the first component)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question)** Now, apply EnKS changing the values of $Q$ and $R$. First instance, increase significantly the observation or the model covariances and let's see whats happen. What are the impacts on the estimated mean ($x^s$) and covariance ($P^s$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bad specified covariances, the EnKS can diverge and this is the worst issue in data assimilation... To avoid this problem, we use the batch EM to estimate $Q$ and $R$ covariances. First, we initialize the state-space model with realistic $Q^{(0)}$ and $R^{(0)}$ values. Then, we run the EM algorithm with a given number of iterations.\n",
    "\n",
    "[Puede ser que tarda mucho, pueden tomar un maté...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 100 # number of EM iterations\n",
    "xb, B = climat_background(x_true) # background state\n",
    "\n",
    "# parameters\n",
    "params = { 'initial_background_state'                 : xb,\n",
    "           'initial_background_covariance'            : B,\n",
    "           'initial_model_noise_covariance'           : eye(3),\n",
    "           'initial_observation_noise_covariance'     : eye(3),\n",
    "           'model_dynamics'                           : m,\n",
    "           'observation_matrix'                       : H,\n",
    "           'observations'                             : y,\n",
    "           'nb_particles'                             : 100,\n",
    "           'nb_EM_iterations'                         : N_iter,\n",
    "           'true_state'                               : x_true,\n",
    "           'inflation_factor'                         : 1,\n",
    "           'temporal_window_size'                     : K,\n",
    "           'state_size'                               : 3,\n",
    "           'observation_size'                         : 3,\n",
    "           'is_model_noise_covariance_estimated'      : True,\n",
    "           'is_observation_noise_covariance_estimated': True,\n",
    "           'is_background_estimated'                  : True,\n",
    "           'model_noise_covariance_structure'         : 'full'}\n",
    "\n",
    "# function\n",
    "res_EM_EnKS = EM_EnKS(params, RandomState(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the mean diagonal terms of $Q$ and $R$ along the iterations. We also plot the evolution of the likelihood and of the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract outputs\n",
    "Q_est=res_EM_EnKS['EM_model_noise_covariance']\n",
    "R_est=res_EM_EnKS['EM_observation_noise_covariance']\n",
    "loglik_est=res_EM_EnKS['loglikelihood']\n",
    "RMSE_est=res_EM_EnKS['RMSE']\n",
    "\n",
    "# plot trace of Q\n",
    "subplot(2,2,1)\n",
    "line1,=plot(trace(Q_est)/3,'r')\n",
    "line2,=plot((1,N_iter),(trace(Q_true)/3,trace(Q_true)/3),'--k')\n",
    "title('Q estimates', fontsize=20)\n",
    "\n",
    "# plot trace of R\n",
    "subplot(2,2,2)\n",
    "line1,=plot(trace(R_est)/3,'r')\n",
    "line2,=plot((1,N_iter),(trace(R_true)/3,trace(R_true)/3),'--k')\n",
    "title('R estimates', fontsize=20)\n",
    "\n",
    "# plot log-likelihood\n",
    "subplot(2,2,3)\n",
    "plot(loglik_est,'r')\n",
    "title('Log-likelihood', fontsize=20)\n",
    "\n",
    "# plot Root Mean Square Error\n",
    "subplot(2,2,4)\n",
    "plot(RMSE_est,'r')\n",
    "title('RMSE', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question)** Now, you are an expert! Play with the EM algorithm changing the values of the important parameters like *dt_obs* (the sampling of the observations) and *K* (the number of time steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
